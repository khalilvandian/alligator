{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\pandas\\__init__.py\", line 39, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 17, in <module>\n",
      "    from pandas.compat._constants import (\n",
      "ImportError: cannot import name 'ISMUSL' from 'pandas.compat._constants' (b:\\Software\\Anaconda\\Lib\\site-packages\\pandas\\compat\\_constants.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\BlackDEATH\\AppData\\Local\\Temp\\ipykernel_3216\\16543489.py\", line 1, in <module>\n",
      "    import pandas\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\pandas\\__init__.py\", line 44, in <module>\n",
      "    raise ImportError(\n",
      "ImportError: C extension: pandas.compat._constants not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"b:\\Software\\Anaconda\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github Data Exploration\n",
    "In this file, the dataset gathered from github repos will be explored for different aspects and insights regarding the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/github.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset's shape is: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format and Range\n",
    "- only id has a minimum charachter count more than 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_char_counts = data.apply(lambda x: x.astype(str).str.len().min())\n",
    "\n",
    "print(f\"The minimum character counts are:\")\n",
    "print(min_char_counts.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char_counts = data.apply(lambda x: x.astype(str).str.len().max())\n",
    "\n",
    "print(f\"The maximum character counts are:\")\n",
    "print(max_char_counts.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded list of non-numeric columns identified from the notebook\n",
    "non_numeric_columns = [\"name\", \"bio\", \"website\", \"company\", \"description\", \"readme\"]\n",
    "\n",
    "# Function to calculate the word count for each entry in specified columns\n",
    "def calculate_word_count_range(data, columns):\n",
    "    word_count_ranges = {}\n",
    "    for col in columns:\n",
    "        word_counts = data[col].dropna().apply(lambda x: len(str(x).split()))\n",
    "        word_count_ranges[col] = {\n",
    "            'min': word_counts.min(),\n",
    "            'max': word_counts.max(),\n",
    "            'range': word_counts.max() - word_counts.min()\n",
    "        }\n",
    "    return word_count_ranges\n",
    "\n",
    "# Calculate and display the word count range for each specified column\n",
    "word_count_ranges = calculate_word_count_range(data, non_numeric_columns)\n",
    "for col, stats in word_count_ranges.items():\n",
    "    print(f\"{col}: Min words = {stats['min']}, Max words = {stats['max']}, Range = {stats['range']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the word count distribution for 'bio' and 'company'\n",
    "def plot_word_count_distribution(data, columns):\n",
    "    for col in columns:\n",
    "        word_counts = data[col].dropna().apply(lambda x: len(str(x).split()))\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(word_counts, bins=40, edgecolor='black', alpha=0.7)\n",
    "        plt.title(f'Word Count Distribution for {col}')\n",
    "        plt.xlabel('Word Count')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "# Call the function for 'bio' and 'company' columns\n",
    "plot_word_count_distribution(data, ['bio', 'company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process words in a column for word count and word cloud\n",
    "def analyze_text_column(data, column, show_wordcloud=False, display_top_n=10):\n",
    "    # Join all text entries in the column and split into words\n",
    "    text = ' '.join(data[column].dropna())\n",
    "    words = text.split()  # Tokenize; can add more sophisticated tokenization if needed\n",
    "    word_counts = Counter(words)  # Count words\n",
    "    \n",
    "    # Convert counts to a Series sorted in descending order\n",
    "    word_count_series = pd.Series(word_counts).sort_values(ascending=False)\n",
    "    \n",
    "    # Display top N words\n",
    "    print(f\"Top {display_top_n} words in {column}:\\n\", word_count_series.head(display_top_n))\n",
    "    \n",
    "    # Generate and display word cloud if requested\n",
    "    if show_wordcloud:\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=False).generate(text)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Word Cloud for {column}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return word_count_series\n",
    "\n",
    "# Run the analysis for 'bio' and 'company' with word clouds\n",
    "bio_word_counts = analyze_text_column(data, 'bio', show_wordcloud=True)\n",
    "company_word_counts = analyze_text_column(data, 'company', show_wordcloud=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_word_counts.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Significant number of rows that are missing the data about website and company. Description, Readme and name are the columns with very low to none missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idd                 0\n",
       "name                1\n",
       "bio            137386\n",
       "website        292797\n",
       "company        340315\n",
       "id                  0\n",
       "description        12\n",
       "readme           2258\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_count = data.isna().sum()\n",
    "na_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of missing values\n",
    "na_percentage = na_count / len(data) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "Dropping columns id and idd, 291 duplicates are found.\n",
    "Dropping only idd, there is 18 duplicates, accopanying that with also dropping id, 291 duplicates.\n",
    "- idd is unique for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for the github dataset is: 561932\n",
      "duplicate count without dropping any column is: 0\n",
      "Duplicate count for all columns except id: 0\n",
      "Duplicate count for all columns except idd: 18\n",
      "Duplicate count for all columns except id and name: 18\n",
      "Duplicate count for all columns except idd and id: 291\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows for the github dataset is: {data.shape[0]}\")\n",
    "\n",
    "temp = data.shape[0] - data.drop_duplicates().shape[0]\n",
    "print(f\"duplicate count without dropping any column is: {temp}\")\n",
    "\n",
    "temp = data.shape[0] - data.drop(columns=[\"id\"]).drop_duplicates().shape[0]\n",
    "print(f\"Duplicate count for all columns except id: {temp}\")\n",
    "\n",
    "temp = data.shape[0] - data.drop(columns=[\"idd\"]).drop_duplicates().shape[0]\n",
    "print(f\"Duplicate count for all columns except idd: {temp}\")\n",
    "\n",
    "temp = data.shape[0] - data.drop(columns=[\"idd\", \"id\"]).drop_duplicates().shape[0]\n",
    "print(f\"Duplicate count for all columns except idd and id: {temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at an example of duplicates in id column, we notice the rows are absolutey the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        id |   count |\n",
      "|----------:|--------:|\n",
      "| 703291877 |       3 |\n",
      "| 701887244 |       3 |\n",
      "| 698887842 |       3 |\n",
      "| 698919769 |       2 |\n",
      "| 643330227 |       2 |\n",
      "\n",
      "rows with id 703291877 is:\n",
      "   idd name                                bio website company        id                        description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  readme\n",
      " 27564 MKAI Graduate of Strathclyde University     NaN     NaN 703291877 MarioKart AI using Deep Q learning ï»¿# MarioKart AI for MarioKart: Double Dash\\r\\n___\\r\\nThis work is completed in partnership with [Alessandro](https://github.com/theseafaringturtle) and would not be possible without him and this expert knowledge of reverse engineering teaching me his ways.\\r\\n___\\r\\n## Installation Guide\\r\\n\\r\\nTODO\\r\\n___\\r\\n## Aims of this project\\r\\n\\r\\nI had a few aims when coming up with this project and these are the inital ones\\r\\n\\r\\n- Make an AI that can beat out the internal Mario Kart bots\\r\\n- Have a clean and easy to understand codebase\\r\\n  - While there are github repositories already doing this with MarioKart Wii such as [this](https://github.com/VIPTankz/OldWiiRL) it is old and confusing to read\\r\\n- Learn how to start with just a game and build an AI out of this\n",
      " 28938 MKAI Graduate of Strathclyde University     NaN     NaN 703291877 MarioKart AI using Deep Q learning ï»¿# MarioKart AI for MarioKart: Double Dash\\r\\n___\\r\\nThis work is completed in partnership with [Alessandro](https://github.com/theseafaringturtle) and would not be possible without him and this expert knowledge of reverse engineering teaching me his ways.\\r\\n___\\r\\n## Installation Guide\\r\\n\\r\\nTODO\\r\\n___\\r\\n## Aims of this project\\r\\n\\r\\nI had a few aims when coming up with this project and these are the inital ones\\r\\n\\r\\n- Make an AI that can beat out the internal Mario Kart bots\\r\\n- Have a clean and easy to understand codebase\\r\\n  - While there are github repositories already doing this with MarioKart Wii such as [this](https://github.com/VIPTankz/OldWiiRL) it is old and confusing to read\\r\\n- Learn how to start with just a game and build an AI out of this\n",
      "157921 MKAI Graduate of Strathclyde University     NaN     NaN 703291877 MarioKart AI using Deep Q learning ï»¿# MarioKart AI for MarioKart: Double Dash\\r\\n___\\r\\nThis work is completed in partnership with [Alessandro](https://github.com/theseafaringturtle) and would not be possible without him and this expert knowledge of reverse engineering teaching me his ways.\\r\\n___\\r\\n## Installation Guide\\r\\n\\r\\nTODO\\r\\n___\\r\\n## Aims of this project\\r\\n\\r\\nI had a few aims when coming up with this project and these are the inital ones\\r\\n\\r\\n- Make an AI that can beat out the internal Mario Kart bots\\r\\n- Have a clean and easy to understand codebase\\r\\n  - While there are github repositories already doing this with MarioKart Wii such as [this](https://github.com/VIPTankz/OldWiiRL) it is old and confusing to read\\r\\n- Learn how to start with just a game and build an AI out of this\n"
     ]
    }
   ],
   "source": [
    "print(data[\"id\"].value_counts().sort_values(ascending=False).head(5).to_markdown())\n",
    "print()\n",
    "print(\"rows with id 703291877 is:\")\n",
    "print(data[data[\"id\"] == 703291877].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at rows that are not duplicate without idd column, but without both idd and id, are considered so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "701887244    2\n",
       "703291877    2\n",
       "698887842    2\n",
       "262609246    1\n",
       "251368830    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idd_id_dups = data[data.drop(columns=[\"idd\", \"id\"]).duplicated()]\n",
    "idd_id_dups[\"id\"].value_counts().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows that have duplicates for non id + idd columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " idd                         name bio website company        id                                                                                                                         description                                                                                                                                                                                                                                                                                                                             readme\n",
      " 570 Stock-Market-Price-Predictor NaN     NaN     NaN 524396620 This is an attempt to make a Machine Learning Model which predicts the Future Stock Market Price using Linear Regression Algorithm. # Stock-Market-Price-Predictor\\r\\nThis is an attempt to make a Machine Learning Model which predicts the Future Stock Market Price using Linear Regression Algorithm.\\r\\n\\r\\nThis approach is not valid since here the data is not standardized and hence the prediction obtained is biased.\\r\\n\\r\\nRefer STATUS.md for more info.\n",
      " 678     practicalmachinelearning NaN     NaN     NaN  84642299                                                                              Repository for JHU Coursera Practical Machine Learning                                                                                                                                                                                                                                               # practicalmachinelearning\\r\\nRepository for JHU Coursera Practical Machine Learning\n",
      " 709      Artificial-Intelligence NaN     NaN     NaN 604892036                                                                                                             Artificial Intelligence                                                                                                                                                                                                                                                                               # Artificial-Intelligence\\r\\nArtificial Intelligence\n",
      "1563     practicalmachinelearning NaN     NaN     NaN  83708234                                                                              Repository for JHU Coursera Practical Machine Learning                                                                                                                                                                                                                                               # practicalmachinelearning\\r\\nRepository for JHU Coursera Practical Machine Learning\n",
      "2146              fraud_detection NaN     NaN     NaN 515266302                                                                               Building a Machine Learning model for fraud detection                                                                                                                                                                                                                                                                                                                  # fraud_detection\n",
      "\n",
      "Shape of the dataset is: (431, 8)\n"
     ]
    }
   ],
   "source": [
    "wanted_rows_id_idd = data[data.drop(columns=[\"idd\", \"id\"]).duplicated(keep=False)]\n",
    "print(wanted_rows_id_idd.head().to_string(index=False))\n",
    "\n",
    "#print shape\n",
    "print()\n",
    "print(f\"Shape of the dataset is: {wanted_rows_id_idd.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows that have dups for non idd columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  idd                                                name                                                                                                                               bio                                  website            company        id                                                                                                                                                                                             description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               readme\n",
      " 9955                                       SignExtractor                                    Currently pursuing Master in Mechatronics with focus on Machine Learning and Computer Vision.  https://www.linkedin.com/in/nishil-balar Universität Siegen 698887842                                                                                                            A mini project work to extract sign from scanned documment using image processing techniques # SignExtractor\\r\\nA mini project work to extract sign from scanned documment using image processing techniques. This project provides basic signature exraction from high quality scanned document\\r\\n\\r\\n  The 4 stage followed to fulfill task is:\\r\\n\\r\\n- [Page Cropping - Transformation of perspective](https://github.com/NishilBalar/SignExtractor/blob/master/crop_edge.py)\\r\\n- [Signatre extraction](https://github.com/NishilBalar/SignExtractor/blob/master/sign_extractor.py)\\r\\n- [Softening mask](https://github.com/NishilBalar/SignExtractor/blob/master/soften.py)\\r\\n- [Color adjustion](https://github.com/NishilBalar/SignExtractor/blob/master/color_adjust.py)\\r\\n\\r\\n## Example of application of algorithm to extract signature\\r\\n\\r\\n- Input = The digital photo of the document (scanned from mobile or printer) \\r\\n- Output = The signatures exist on the input\\r\\n\\r\\n  Note: Here, I used my experience certificate as input image which is being scanned from mobile camera. Output with extracted image is shown below.\\r\\n\\r\\n<p align=\"center\">\\r\\n  <img src=\"https://github.com/NishilBalar/SignExtractor/blob/master/result.gif\" | width=450>\\r\\n</p>\\r\\n\\r\\n**Summary:** Firstly, the page cropping algorithm is performed to transform scanned document to page perspective. After that, Signatre extraction algorithm is being performed to extract sign from scanned documents. At last, softening and colour adjustment algorithm are performed to get output document with high quality! \\r\\n\\r\\n\\r\\n## Installation & Run\\r\\n\\r\\n**1.) pip library installation**\\r\\n\\r\\n- Python version requirements: 3.3+\\r\\n\\r\\nCreate virtual environment and install all required dependencies as stated in requirement.txt file as follow in command prompt\\r\\n\\r\\n```\\r\\npython -m venv .venv\\r\\n\\r\\n.venv\\Scripts\\activate\\r\\n\\r\\npip install -r requirements.txt\\r\\n\\r\\n```\\r\\n\\r\\n**2.) Run code to extract signature**\\r\\nAfter adding your desired image as `test.jpg`, run following command in command prompt to run signature extraction algorithm!\\r\\n\\r\\n```\\r\\npython -m magic\\r\\n\\r\\n```\\r\\n\\r\\n## Acknowledgement\\r\\n    @ONLINE{hse,\\r\\n            author = \"Ahmet ÃzlÃ¼\",\\r\\n            title  = \"Overlapped handwritten signature extraction from scanned documents\",\\r\\n            year   = \"2018\",\\r\\n            url    = \"https://github.com/ahmetozlu/signature_extractor\"\\r\\n        }\\r\\n\\r\\n\\r\\n\n",
      "27564                                                MKAI                                                                                                Graduate of Strathclyde University                                      NaN                NaN 703291877                                                                                                                                                                      MarioKart AI using Deep Q learning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ï»¿# MarioKart AI for MarioKart: Double Dash\\r\\n___\\r\\nThis work is completed in partnership with [Alessandro](https://github.com/theseafaringturtle) and would not be possible without him and this expert knowledge of reverse engineering teaching me his ways.\\r\\n___\\r\\n## Installation Guide\\r\\n\\r\\nTODO\\r\\n___\\r\\n## Aims of this project\\r\\n\\r\\nI had a few aims when coming up with this project and these are the inital ones\\r\\n\\r\\n- Make an AI that can beat out the internal Mario Kart bots\\r\\n- Have a clean and easy to understand codebase\\r\\n  - While there are github repositories already doing this with MarioKart Wii such as [this](https://github.com/VIPTankz/OldWiiRL) it is old and confusing to read\\r\\n- Learn how to start with just a game and build an AI out of this\n",
      "28938                                                MKAI                                                                                                Graduate of Strathclyde University                                      NaN                NaN 703291877                                                                                                                                                                      MarioKart AI using Deep Q learning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ï»¿# MarioKart AI for MarioKart: Double Dash\\r\\n___\\r\\nThis work is completed in partnership with [Alessandro](https://github.com/theseafaringturtle) and would not be possible without him and this expert knowledge of reverse engineering teaching me his ways.\\r\\n___\\r\\n## Installation Guide\\r\\n\\r\\nTODO\\r\\n___\\r\\n## Aims of this project\\r\\n\\r\\nI had a few aims when coming up with this project and these are the inital ones\\r\\n\\r\\n- Make an AI that can beat out the internal Mario Kart bots\\r\\n- Have a clean and easy to understand codebase\\r\\n  - While there are github repositories already doing this with MarioKart Wii such as [this](https://github.com/VIPTankz/OldWiiRL) it is old and confusing to read\\r\\n- Learn how to start with just a game and build an AI out of this\n",
      "62952 Streamlit-Based_Crop_Nutrient_Deficiency_Classifier <U+0001F44B> Hi there! I'm Shailendra Singh, and I'm passionate about data analysis, machine learning, and web development.\\r\\r\\n     https://ssinghportfolio.netlify.app/                NaN 698830173 This is a web application that simplifies the identification of plant nutrient deficiencies. Using just a picture of a plant's leaf, this user-friendly tool predicts nutrient deficiencies accurately.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  # Streamlit-Based_Crop_Nutrient_Deficiency_Classifier\\r\\nNutriScan: Crop Health Insights Web App\\r\\nNutriScan is a web application that aids farmers and agricultural enthusiasts in crop health monitoring and nutrient deficiency diagnosis. This user-friendly tool leverages deep learning and offers a streamlined interface for easy image analysis.\\r\\n\\r\\nKey Features\\r\\nStreamlit Interface: NutriScan's intuitive Streamlit-based interface ensures accessibility for users of all backgrounds.\\r\\n\\r\\nImage Upload and Analysis: Users can effortlessly upload crop images, which the application analyzes to identify nutrient deficiencies.\\r\\n\\r\\nDeep Learning Model: The app employs a Convolutional Neural Network (CNN) model built with TensorFlow and Keras. It recognizes four categories: Phosphorus Deficiency, Nitrogen Deficiency, Potassium Deficiency, and Healthy Plants.\\r\\n\\r\\nInformative Explanations: NutriScan provides detailed, collapsible explanations for each deficiency, including effects and recommended actions.\\r\\n\\r\\nBackground Customization: Users can personalize the app by setting custom background images.\\r\\n\\r\\nPerformance Optimization: The code optimizes performance with caching mechanisms for efficient model loading and prediction.\\r\\n\\r\\nHow to Use\\r\\nVisit the NutriScan web app.\\r\\nUpload a crop image for analysis.\\r\\nClick \"Predict\" to identify nutrient deficiencies.\\r\\nExplore the explanations and recommendations.\n",
      "96736                                datascience-hands-on                                                                                                                               NaN                                      NaN                NaN 698844365           \"My data science practice space  where I work on projects to learn and get better at data analysis and machine learning.  Feel free to leave comments and join me on this learning journey!\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  # datascience-hands-on\\r\\nMy data science practice space â where I work on projects to learn and get better at data analysis and machine learning. Feel free to leave comments and join me on this learning journey!\\r\\n### [Uber-analysis notebook](https://github.com/Keerthiga-V-N-R/datascience-hands-on/blob/main/Uber_analysis.ipynb) - [csv file](https://github.com/Keerthiga-V-N-R/datascience-hands-on/blob/main/My%20Uber%20Drives%20-%202016.csv)\\r\\n### [IMDb Project with SQL](https://github.com/Keerthiga-V-N-R/datascience-hands-on/tree/main/IMDb_Project(SQL))\n",
      "\n",
      "Shape of the dataset is: (33, 8)\n"
     ]
    }
   ],
   "source": [
    "wanted_rows_idd = data[data.drop(columns=[\"idd\"]).duplicated(keep=False)]\n",
    "print(wanted_rows_idd.head().to_string(index=False))\n",
    "\n",
    "#print shape\n",
    "print()\n",
    "print(f\"Shape of the dataset is: {wanted_rows_idd.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find shared rows for the 2 to check if one is inclusive, => all rows in wanted_rows_idd are included in wanted_rows_idd_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_dup_rows = wanted_rows_idd.merge(wanted_rows_id_idd, on=[\"id\", \"idd\"], how=\"inner\")\n",
    "shared_dup_rows.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if these mutual values have 18 dups for non idd columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_dup_rows.shape[0] - shared_dup_rows.drop(columns=[\"idd\"]).drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now find the non shared ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idd       name bio website company        id                     description                                          readme\n",
      "424890        480 NaN     NaN     NaN 146799683                           IA480                                  # 480\\r\\nIA480\n",
      "372081        480 NaN     NaN     NaN 146799699                           IA480                                  # 480\\r\\nIA480\n",
      "  5025         AI NaN     NaN     NaN  83171295         Artificial Intelligence                 # AI\\r\\nArtificial Intelligence\n",
      "381957         AI NaN     NaN     NaN 385212180         Artificial Intelligence                 # AI\\r\\nArtificial Intelligence\n",
      "396183 AI-Project NaN     NaN     NaN 302144136 Artificial Intelligence Project # AI-Project\\r\\nArtificial Intelligence Project\n"
     ]
    }
   ],
   "source": [
    "# left anti join to get the rows that are not shared\n",
    "non_shared_dup_rows = wanted_rows_id_idd[~wanted_rows_id_idd[\"id\"].isin(wanted_rows_idd[\"id\"])]\n",
    "\n",
    "print(non_shared_dup_rows.sort_values(by=[\"name\"]).head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of dups which must be 219 - 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_shared_dup_rows.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_shared_dup_rows.drop(columns=[\"idd\", \"id\"]).drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_shared_dup_rows.shape[0] - non_shared_dup_rows.drop(columns=[\"idd\", \"id\"]).drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_mask_idd_id = data.drop(columns=[\"idd\", \"id\"]).duplicated(keep=False)\n",
    "\n",
    "# extract all rows that have duplicates based on all columns except 'idd' and 'id'\n",
    "temp_wanted = data[duplicates_mask_idd_id]\n",
    "temp_wanted.shape[0]\n",
    " \n",
    "# test if these rows have only 18 dup when we drop 'idd' column\n",
    "temp_wanted.drop(columns=[\"idd\"]).drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_wanted.drop(columns=[\"idd\", \"id\"]).drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "561927    False\n",
       "561928    False\n",
       "561929    False\n",
       "561930    False\n",
       "561931    False\n",
       "Length: 561932, dtype: bool"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=[\"idd\", \"id\"]).duplicated(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = data.drop(columns=[\"idd\", \"id\"]).duplicated().sum()\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a proxy for users, we group the data with bio + website + company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is estimated that there are 199371 unique users out of  561932  rows.\n"
     ]
    }
   ],
   "source": [
    "# unique values of the combination of the columns bio, website and company\n",
    "users = data.loc[:, [\"bio\", \"website\", \"company\"]]\n",
    "unique_users = users.drop_duplicates()\n",
    "\n",
    "print(\"It is estimated that there are\", unique_users.shape[0], \"unique users out of \", users.shape[0],\" rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bio                                                                                                                                                                                                                                        website                                   company                              \n",
       "\\r\\r\\n                                                                                                                                                                                                                                     1653408133@qq.com                         China University of Geosciences,Wuhan    1\n",
       "                                                                                                                                                                                                                                           alemel.is                                 @vivacitylabs                            1\n",
       "                                                                                                                                                                                                                                           borisov1990@gmail.com                     +359 897 79 25 88                        1\n",
       "                                                                                                                                                                                                                                           cityloop.cc                               Cityloop                                 1\n",
       "                                                                                                                                                                                                                                           dataisland.org                            Northwestern University                  1\n",
       "                                                                                                                                                                                                                                                                                                                             ..\n",
       "Ærospace & Robotics Eng....   by <U+0001D625><U+0001D622><U+0001D63A>\\r\\r\\nCoder & DL/CV Researcher...  by <U+0001D48F><U+0001D48A><U+0001D488><U+0001D489><U+0001D495>                               $oftwre ngineer working in Finance  https://www.linkedin.com/in/xavier-goby/  ACTRegTech                               1\n",
       "É tudo normar <U+0001F920><U+200A><U+0001F44D><U+200A>                                                                                                                                                                                     https://compjunior.com.br/                Comp Júnior                              1\n",
       "École Polytechnique Paris Master student in Artificial Intelligence & Advanced Visual Computing. McGill University Engineering Graduate.                                                                                                   http://www.linkedin.com/in/natepollet     Ecole Polytechnique                      2\n",
       "ß Level @MicrosoftStudentPartners | @nasa Space Apps Challenge Winner | @harvard X Community TA.\\r\\r\\nComputer Science Engineer<U+0001F4BB>; Loves to code<U+0001F431><U+200D><U+0001F4BB>; Write a bit<U+0001F58B>                        https://prankshaw.github.io/              @Intuit                                  2\n",
       "üsküdar american academy                    -           \\r\\r\\n\\r\\r\\nmiddle east technical university                                                                                                                                       demegire.github.io                        @odtuyzt                                 1\n",
       "Length: 44024, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group the data by 3 columns and count the number of rows in each group\n",
    "grouped = data.groupby([\"bio\", \"website\", \"company\"]).size()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A closer look at rows with comany name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that have a value for their company are a priority, exploration is focused on these rows for the initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The remaining data's shape is: (221617, 8)\n"
     ]
    }
   ],
   "source": [
    "filtered_data = data[data['company'].notna()]\n",
    "print(f\"The remaining data's shape is: {filtered_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idd</th>\n",
       "      <th>name</th>\n",
       "      <th>bio</th>\n",
       "      <th>website</th>\n",
       "      <th>company</th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>readme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Algorithms-DataStructures-Python</td>\n",
       "      <td>Former Deep Learning Intern |  Lead Programmer...</td>\n",
       "      <td>https://www.callitabhi.com/</td>\n",
       "      <td>CallitAbhi</td>\n",
       "      <td>672799594</td>\n",
       "      <td>Welcome to my Algorithms and Data Structures r...</td>\n",
       "      <td># Algorithms-DataStructures-Python\\r\\n\\r\\n- `b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Hindi-character-recognition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sarvajanik College of Engineering &amp; Technology</td>\n",
       "      <td>667535261</td>\n",
       "      <td>In this work, we propose a technique to recogn...</td>\n",
       "      <td># Hindi-character-recognition\\r\\nIn this work,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>plant_leaf_disease_detection</td>\n",
       "      <td>Machine learning &amp; Deep Learning Practitioner....</td>\n",
       "      <td>https://abhinav3.github.io/</td>\n",
       "      <td>IIT Guwahati</td>\n",
       "      <td>692408006</td>\n",
       "      <td>plant leaf disease detection</td>\n",
       "      <td>plant leaf disease detection and classificatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>tello_python</td>\n",
       "      <td>IoT | Deep Learning | Machine Learning | Embed...</td>\n",
       "      <td>https://www.linkedin.com/in/adithya-u-r-795866...</td>\n",
       "      <td>KTH Royal Insitute Of Technology</td>\n",
       "      <td>185018601</td>\n",
       "      <td>Gesture based control of Tello drone using python</td>\n",
       "      <td># TelloSDKPy\\r\\nDJI Tello drone python interfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Deep-Learning-Projects</td>\n",
       "      <td>Machine Learning Enthusiast and loves to innov...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>431716441</td>\n",
       "      <td>It contains Deep-Learning projects designed on...</td>\n",
       "      <td># Deep-Learning-Projects\\r\\nIt contains Deep-L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idd                              name  \\\n",
       "0     1  Algorithms-DataStructures-Python   \n",
       "8     9       Hindi-character-recognition   \n",
       "11   12      plant_leaf_disease_detection   \n",
       "15   16                      tello_python   \n",
       "17   18            Deep-Learning-Projects   \n",
       "\n",
       "                                                  bio  \\\n",
       "0   Former Deep Learning Intern |  Lead Programmer...   \n",
       "8                                                 NaN   \n",
       "11  Machine learning & Deep Learning Practitioner....   \n",
       "15  IoT | Deep Learning | Machine Learning | Embed...   \n",
       "17  Machine Learning Enthusiast and loves to innov...   \n",
       "\n",
       "                                              website  \\\n",
       "0                         https://www.callitabhi.com/   \n",
       "8                                                 NaN   \n",
       "11                        https://abhinav3.github.io/   \n",
       "15  https://www.linkedin.com/in/adithya-u-r-795866...   \n",
       "17                                                NaN   \n",
       "\n",
       "                                           company         id  \\\n",
       "0                                       CallitAbhi  672799594   \n",
       "8   Sarvajanik College of Engineering & Technology  667535261   \n",
       "11                                    IIT Guwahati  692408006   \n",
       "15                KTH Royal Insitute Of Technology  185018601   \n",
       "17                       Tata Consultancy Services  431716441   \n",
       "\n",
       "                                          description  \\\n",
       "0   Welcome to my Algorithms and Data Structures r...   \n",
       "8   In this work, we propose a technique to recogn...   \n",
       "11                       plant leaf disease detection   \n",
       "15  Gesture based control of Tello drone using python   \n",
       "17  It contains Deep-Learning projects designed on...   \n",
       "\n",
       "                                               readme  \n",
       "0   # Algorithms-DataStructures-Python\\r\\n\\r\\n- `b...  \n",
       "8   # Hindi-character-recognition\\r\\nIn this work,...  \n",
       "11  plant leaf disease detection and classificatio...  \n",
       "15  # TelloSDKPy\\r\\nDJI Tello drone python interfa...  \n",
       "17  # Deep-Learning-Projects\\r\\nIt contains Deep-L...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except bio and website, the other value are not missing a significant number of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idd                0\n",
       "name               1\n",
       "bio            22878\n",
       "website        81395\n",
       "company            0\n",
       "id                 0\n",
       "description        4\n",
       "readme           890\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 56000 unique company names _ no preprocessing applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56429"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_data[\"company\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique count of companies are 1/4 of the rows count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25462396837787715"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_data[\"company\"].unique())/(filtered_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique values for company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are obvious companies and institutions among the unique companies, but also there are values such as student and freelancer which are irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company\n",
       "Microsoft                            1552\n",
       "Amazon                               1062\n",
       "Google                                986\n",
       "Student                               841\n",
       "Carnegie Mellon University            568\n",
       "IBM                                   531\n",
       "Northeastern University               435\n",
       "Columbia University                   435\n",
       "Freelancer                            418\n",
       "New York University                   400\n",
       "Accenture                             394\n",
       "Stanford University                   391\n",
       "University of Toronto                 380\n",
       "NVIDIA                                379\n",
       "Imperial College London               373\n",
       "University of Cambridge               366\n",
       "University of Southern California     363\n",
       "Tata Consultancy Services             354\n",
       "Tsinghua University                   337\n",
       "Amazon Web Services                   327\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[\"company\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are noisy values and values with encoding problems in the data\n",
    "- \"<U+043D><U+0443><U+043B><U+043B>\"\n",
    "-  @deezer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561882                                 Confluent \n",
       "561886        NYU Schack Institute of Real Estate\n",
       "561889                  The University of Alabama\n",
       "561892                     University of Burgundy\n",
       "561893                                      Baidu\n",
       "561898                  Grras Solutions Pvt Ltd. \n",
       "561901                                      ONERA\n",
       "561902                                       Flix\n",
       "561905                                  Blackrock\n",
       "561906    Societe Generale Global Solution Centre\n",
       "561910                                 SK Telecom\n",
       "561911           <U+043D><U+0443><U+043B><U+043B>\n",
       "561912                               @huggingface\n",
       "561914                 National Taiwan University\n",
       "561916                                   @deezer \n",
       "561919                                 NUDT&SWUFE\n",
       "561921                Sapienza University of Rome\n",
       "561924                   Johns Hopkins University\n",
       "561925             Kochi University of Technology\n",
       "561929                      LIBRA AI Technologies\n",
       "Name: company, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[\"company\"].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    56429.000000\n",
       "mean         3.927360\n",
       "std         15.492701\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          3.000000\n",
       "max       1552.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts = filtered_data[\"company\"].value_counts()\n",
    "\n",
    "# statistical parameters of the value counts\n",
    "value_counts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company Column Quality assessment\n",
    "A first glance at issues that lower the quality of the company column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase/Uppercase and Trimming\n",
    "1102 reduction in unique values is achieved with trimming and lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reduced unique count is: 3321\n"
     ]
    }
   ],
   "source": [
    "processed_companies = filtered_data[\"company\"]\n",
    "\n",
    "# transform the company column to lowercase\n",
    "processed_companies = processed_companies.str.lower()\n",
    "\n",
    "# trim the company column\n",
    "processed_companies = processed_companies.str.strip()\n",
    "\n",
    "reduced_unique_count = len(filtered_data[\"company\"].unique()) - len(processed_companies.unique())\n",
    "\n",
    "print(f\"The reduced unique count is: {reduced_unique_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non alphebatic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique companies with non-alphabetical characters is: 24473\n",
      "\n",
      "| company                         |   count |\n",
      "|:--------------------------------|--------:|\n",
      "| @microsoft                      |     717 |\n",
      "| @google                         |     519 |\n",
      "| self-employed; looking for tips |     259 |\n",
      "| @nvidia                         |     231 |\n",
      "| @huggingface                    |     204 |\n",
      "| @facebook                       |     195 |\n",
      "| @aws                            |     168 |\n",
      "| texas a&m university            |     144 |\n",
      "| self-employed                   |     125 |\n",
      "| university of wisconsin-madison |     122 |\n",
      "| eth zürich                      |     115 |\n",
      "| @amzn                           |     115 |\n",
      "| thingtrack s.l. (freelance)     |     112 |\n",
      "| @ibm                            |     110 |\n",
      "| @facultyai                      |     107 |\n",
      "| @flamesllc                      |     105 |\n",
      "| king's college london           |      96 |\n",
      "| university of tübingen          |      95 |\n",
      "| @instar-deutschland             |      94 |\n",
      "| @intel                          |      92 |\n",
      "\n",
      "| company                                 |   count |\n",
      "|:----------------------------------------|--------:|\n",
      "| student at university of nevada, reno   |       1 |\n",
      "| @maum-ai                                |       1 |\n",
      "| @octus-team                             |       1 |\n",
      "| tübitak                                 |       1 |\n",
      "| ph.d. student @ buaa                    |       1 |\n",
      "| abv-iiitm gwalior                       |       1 |\n",
      "| pharmaceutical co.                      |       1 |\n",
      "| cabo main, llc                          |       1 |\n",
      "| @danko-lab                              |       1 |\n",
      "| @paybackgmbh, @alugaqui                 |       1 |\n",
      "| @floteur                                |       1 |\n",
      "| @mediaire                               |       1 |\n",
      "| turkcell iletisim hizmetleri a.s.       |       1 |\n",
      "| cwru -> arizona state university        |       1 |\n",
      "| @blinkist                               |       1 |\n",
      "| brac it services ltd.                   |       1 |\n",
      "| @vintasoftware                          |       1 |\n",
      "| cazoo.co.uk                             |       1 |\n",
      "| sr. technical lead @ a3logics pvt. ltd. |       1 |\n",
      "| nudt&swufe                              |       1 |\n"
     ]
    }
   ],
   "source": [
    "# use regex to find non-alphabetical characters\n",
    "non_alpha_companies = processed_companies[~processed_companies.str.match(r'^[a-z\\s]*$')]\n",
    "\n",
    "print(f\"The number of unique companies with non-alphabetical characters is: {len(non_alpha_companies.unique())}\")\n",
    "print()\n",
    "print(non_alpha_companies.value_counts().head(20).to_markdown())\n",
    "print()\n",
    "print(non_alpha_companies.value_counts().tail(20).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There Are non english letters, encoding issues, symbols within the company data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['æ', 'á', '-', '\\x91', '[', '\\x9e', ']', '\\x99', '@', '.', '\\x92', '0', ';', '%', '·', '&', '3', 'õ', \"'\", 'ü', 'ï', 'ö', '\\\\', '`', '7', '6', '4', 'é', '}', '^', '|', 'â', '®', '\"', 'ô', '/', '»', '#', 'ç', '8', '\\x94', '\\x95', 'ë', 'ø', '<', 'º', '*', '9', '\\x96', '\\x9a', '$', '!', 'ð', '_', 'è', 'å', '=', 'ú', ',', '5', '+', '{', '2', '~', '\\x97', '?', 'ê', ':', '\\x8a', ')', 'ã', 'à', '°', 'í', 'ä', '>', '©', 'ó', '(', '¯', 'ñ', '\\x93', '1', '²', 'ò', '«']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "non_alpha_chars = re.findall(r'[^a-zA-Z\\s]', non_alpha_companies.str.cat(sep=''))\n",
    "non_alpha_chars = list(set(non_alpha_chars))\n",
    "\n",
    "print(non_alpha_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing readme\n",
    "There are almost 800 instances of filtered rows than do not have a readme.\n",
    "\n",
    "No noticeable pattern was found for missing readme and other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows with missing readme data is: 890\n",
      "\n",
      "|      | name                                | company                                           | description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|-----:|:------------------------------------|:--------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  681 | arcada-webdev                       | The Swedish Cultural Foundation in Finland        | A frontend web dev project of my company Layerlinks website for my web dev course at Arcada                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|  754 | Machine-learning-and-data-analytics | Accenture                                         | Complete Data preprocessing, cleaning, variable selections, exloratory data analysis & Regression analysis using Pandas, Numpy, Scikitlearn, and visualization using seaborn libraries                                                                                                                                                                                                                                                                                                       |\n",
      "|  843 | saifulislamsarfaraz.github.io       | East West University-Dhaka, Bangladesh            | Personal Website and Log of Activity                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "| 1419 | Face-Recognison                     | CallitAbhi                                        | face detection                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "| 1653 | charity_project                     | heuristic inc                                     | This project is related to charity  issues                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "| 2217 | FSDS-2-0-Assignments                | Self growth                                       | This is my Respo for all the assignments (Full stack data science 2.0 ineuron)                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "| 2375 | CodeForces                          | @ACM-Amrita-Amritapuri                            | Solutions for problems from CodeForce                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "| 2556 | web3-protocol-diversity             | Atchai                                            | Exploring the diversity of web3 protocols                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "| 3784 | learn_python_the_hard_way           | @m1l0ai                                           | Exercises from python learning                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "| 3951 | mfa-paper                           | LTRI & University of Toronto                      | MFA paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "| 4296 | DeepLearning_BTC                    | U.S. Bancorp                                      | Due to the volatility of cryptocurrency speculation, this project is looking to help build and evaluate deep learning models using the Crypto Fear and Greed Index values and closing prices to determine if the indicator provides a better signal for cryptocurrencies. By using the deep learning recurrent neural network to model bitcoin prices, one model will predict the closing price while the second model will use a window of closing prices to predict the nth closing price. |\n",
      "| 4746 | machine-learning-coursera           | Whatnot                                           | Materials for the Coursera Machine Learning Course                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "| 4839 | Kennissystemen                      | University of Amsterdam                           | Prolog-based Knowledge Systems on toy problems                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "| 5033 | twitter-ner                         | Columbia University                               | deep learning approaches for NER on social media                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "| 6256 | BaysianOptimizationFastAI           | @HuggingFace                                      | Using the Baysian Optimization Library within Fast.AI Working notebook                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "| 6583 | researchexperiment                  | MIT Research Intern / Umass Boston Student Fellow | Testing                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "| 6693 | GameOfLife_SAMPLE                   | Paytrix                                           | [DONE] - Game of life simulation                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "| 7171 | python                              | THOMSON REUTERS                                   | Python                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "| 7875 | streamlit_poc                       | @codefornepal                                     | bare minimum streamlit                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "| 8853 | DeepLearningWithKeras               | Boston University                                 | Deep Learning with Keras, is a Textbook by Francois Chollet, the creator of Keras. This repository if a collection of code samples from the textbook which I will update as I read through it. Code is modified by me towards personalized experimentation with Keras.                                                                                                                                                                                                                       |\n",
      "\n",
      "|        | name                                      | company                                | description                                                                                                                                                                                        |\n",
      "|-------:|:------------------------------------------|:---------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| 548833 | NodeJs                                    | FlandersMake vzw                       | ContactListApp                                                                                                                                                                                     |\n",
      "| 549188 | Text-to-speech-app                        | Annukai Techier                        | Text to speech translation app                                                                                                                                                                     |\n",
      "| 549777 | ML_Projects                               | MuSigma                                | All Machine learning Projects                                                                                                                                                                      |\n",
      "| 550124 | nginx-formula                             | @BoundlessNotions @mitodl              | Saltstack formula for installing Nginx                                                                                                                                                             |\n",
      "| 550832 | HMI-Eurobot                               | Universidad de Alcalá                  | <U+0001F4FA> Interfaz máquina-usuario para los robots de competición del Equipo de Robótica de la Universidad de Alcalá. Compitiendo en Eurobot 2021. Escrito en Python, integrando PyQt5 con ROS. |\n",
      "| 551250 | course-backend                            | @Prunedge-Dev-Team  @zonetechpark      | A Flask REST API application                                                                                                                                                                       |\n",
      "| 551684 | oauth2-auth                               | @Prunedge-Dev-Team  @zonetechpark      | OAuth2 Authentication Using Nodejs and Mongo                                                                                                                                                       |\n",
      "| 552150 | ml.js                                     | ITB, TU Wien                           | Machine learning library in javascript                                                                                                                                                             |\n",
      "| 553890 | rtaData                                   | SELF                                   | Worked on kaggle dataset for the mega guided projects by tmlc group                                                                                                                                |\n",
      "| 554708 | CS5720                                    | Appalachian State University           | A repository for Dr. Parry's Spring 2018 course: Scientific Computing with Visualization                                                                                                           |\n",
      "| 554766 | Codes-CPP                                 | IIIT Lucknow                           | Codes of the problems I solve while doing CP                                                                                                                                                       |\n",
      "| 555022 | nilm.ca                                   | @compsust                              | NILM.ca webpages                                                                                                                                                                                   |\n",
      "| 557552 | CourseraMachineLearning                   | Test Enthu                             | Andrew Ng's course to learn machine learning on Coursera                                                                                                                                           |\n",
      "| 558547 | Drury-Cafe                                | Freelancing                            | Creation of Drury cafe website a company in the hotel business.                                                                                                                                    |\n",
      "| 559448 | sklearn                                   | dedan kimathi university of technology | it consist code on classification on sklearn                                                                                                                                                       |\n",
      "| 559469 | Farm                                      | Freelancing                            | A farm management website.                                                                                                                                                                         |\n",
      "| 559505 | glassboxmedicine                          | Cydoc                                  | Code from my machine learning and medicine blog Glass Box https://glassboxmedicine.com/                                                                                                            |\n",
      "| 559864 | WaterProject                              | heuristic inc                          | This is a simple water project  to track clients payment details                                                                                                                                   |\n",
      "| 560603 | Univeristy-of-Khartoum-Graduation-Project | University of Khartoum                 | Prediction of Regional Biocapacity and Ecological Footprint Using Satellite Imagery and Deep Learning                                                                                              |\n",
      "| 561921 | urbanpark                                 | Sapienza University of Rome            | Android App that allow users to book parking spots in private parking areas. An Human Computer Interaction (User Experience) Project about private parking                                         |\n"
     ]
    }
   ],
   "source": [
    "readme_missing_data = filtered_data[filtered_data[\"readme\"].isna()].loc[:, [\"name\", \"company\", \"description\"]]\n",
    "\n",
    "print(f\"The number of rows with missing readme data is: {readme_missing_data.shape[0]}\")\n",
    "print()\n",
    "print(readme_missing_data.head(20).to_markdown())\n",
    "print()\n",
    "print(readme_missing_data.tail(20).to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
